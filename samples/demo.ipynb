{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.9 64-bit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"interpreter":{"hash":"684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"},"colab":{"name":"demo.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"hipHxzeg0nxz","executionInfo":{"status":"error","timestamp":1628578334152,"user_tz":420,"elapsed":5,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}},"outputId":"bdc5a577-b99e-4fd3-9797-ae68a81708f2"},"source":["import os\n","import sys\n","import random\n","import math\n","import numpy as np\n","import skimage.io\n","import matplotlib\n","import json\n","import skimage\n","import matplotlib.pyplot as plt\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"../\")\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","from mrcnn import utils\n","import mrcnn.model as modellib\n","from mrcnn import visualize\n","# Import COCO config\n","sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n","import coco\n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)\n","\n","# Directory of images to run detection on\n","IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3d966f7638fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Import Mask RCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# To find local version of the library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mrcnn'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"J1XAVKDo0sUI","executionInfo":{"status":"aborted","timestamp":1628578214518,"user_tz":420,"elapsed":876,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcFkSkEA0nx_","executionInfo":{"status":"aborted","timestamp":1628578214520,"user_tz":420,"elapsed":877,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["MODEL_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zay0BEP00nyA","executionInfo":{"status":"aborted","timestamp":1628578214521,"user_tz":420,"elapsed":878,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["class InferenceConfig(coco.CocoConfig):\n","    # Set batch size to 1 since we'll be running inference on\n","    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","config = InferenceConfig()\n","config.display()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8EtqtY80nyB"},"source":["## Create Model and Load Trained Weights"]},{"cell_type":"code","metadata":{"collapsed":true,"scrolled":false,"id":"TDogS9S00nyC","executionInfo":{"status":"aborted","timestamp":1628578214522,"user_tz":420,"elapsed":879,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["# Create model object in inference mode.\n","model = modellib.MaskRCNN(mode=\"training\", model_dir=MODEL_DIR, config=config)\n","\n","# Load weights trained on MS-COCO\n","model.load_weights(COCO_MODEL_PATH, by_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3065tGnq0nyD","executionInfo":{"status":"aborted","timestamp":1628578214522,"user_tz":420,"elapsed":879,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["def get_ax(rows=1, cols=1, size=8):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","    \n","    Change the default size attribute to control the size\n","    of rendered images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gziX4Kh50nyE","executionInfo":{"status":"aborted","timestamp":1628578214524,"user_tz":420,"elapsed":10,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["class CustomDataset(utils.Dataset):\n","\n","    def load_custom(self,dataset_dir, subset,class_t):\n","        self.add_class(\"dent\", 1, \"dent\")\n","        self.add_class(\"scratch\",2,\"scratch\")\n","\n","        # Train or validation dataset?\n","        assert subset in [\"train\", \"val\"]\n","        dataset_dir = os.path.join(dataset_dir, subset)\n","        annotations1 = json.load(open(os.path.join(dataset_dir, \"via_region_data.json\")))\n","        annotations = list(annotations1.values())\n","        annotations = [a for a in annotations if a['regions']]\n","\n","        # Add images\n","        for a in annotations:\n","            polygons = [r['shape_attributes'] for r in a['regions'].values()]\n","            image_path = os.path.join(dataset_dir, a['filename'])\n","            image = skimage.io.imread(image_path)\n","            height, width = image.shape[:2]\n","\n","            self.add_image(\n","                class_t,\n","                image_id=a['filename'],\n","                path=image_path,\n","                width=width, height=height,\n","                polygons=polygons)\n","\n","\n","    def load_mask(self, image_id):\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != 'dent' or image_info[\"source\"] != \"scratch\":\n","            return super(self.__class__, self).load_mask(image_id)\n","        info = self.image_info[image_id]\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n","                        dtype=np.uint8)\n","        for i, p in enumerate(info[\"polygons\"]):\n","            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n","            mask[rr, cc, i] = 1\n","        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n","\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"dent\" or info[\"source\"] == \"scratch\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQfGMyGI0nyG","executionInfo":{"status":"aborted","timestamp":1628578214524,"user_tz":420,"elapsed":10,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["# Training Dent dataset\n","dent_dataset_train = CustomDataset()\n","dent_dataset_train.load_custom(\"D:/lakehead/sem-2/cv/projects&extra/Project/customImages/dent/\",\"train\",\"dent\")\n","dent_dataset_train.prepare()\n","\n","# Training Scratch dataset\n","scratch_dataset_train = CustomDataset()\n","scratch_dataset_train.load_custom(\"D:/lakehead/sem-2/cv/projects&extra/Project/customImages/scratch\",\"train\",\"scratch\")\n","scratch_dataset_train.prepare()\n","\n","# Validation Dent dataset\n","dent_dataset_val = CustomDataset()\n","dent_dataset_val.load_custom(\"D:/lakehead/sem-2/cv/projects&extra/Project/customImages/dent/\",\"val\",\"dent\")\n","dent_dataset_val.prepare()\n","\n","# Validation Scratch dataset\n","scratch_dataset_val = CustomDataset()\n","scratch_dataset_val.load_custom(\"D:/lakehead/sem-2/cv/projects&extra/Project/customImages/scratch/\",\"val\",\"scratch\")\n","scratch_dataset_val.prepare()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CCTcR940nyI","executionInfo":{"status":"aborted","timestamp":1628578214524,"user_tz":420,"elapsed":9,"user":{"displayName":"Spark develop","photoUrl":"","userId":"01748672585390352032"}}},"source":["# *** This training schedule is an example. Update to your needs ***\n","# Since we're using a very small dataset, and starting from\n","# COCO trained weights, we don't need to train too long. Also,\n","# no need to train all layers, just the heads should do it.\n","print(\"Training network heads\")\n","try:\n","    model.train(dent_dataset_train, dent_dataset_val,learning_rate=config.LEARNING_RATE,epochs=10,layers='heads')\n","except Exception:\n","    pass"],"execution_count":null,"outputs":[]}]}